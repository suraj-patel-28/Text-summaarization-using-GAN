{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:14:33.852264Z",
     "start_time": "2020-01-11T18:14:33.843529Z"
    }
   },
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:14:33.865122Z",
     "start_time": "2020-01-11T18:14:33.856877Z"
    }
   },
   "source": [
    "Project repository can be found [here](https://gitlab.com/tankz0r/seminar). It has several branches. Let us see the general structure of the project in Pycharm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras as a framework for the development\n",
    "- CNN dataset\n",
    "- Unpaired WGAN model\n",
    "- 5000 words in the vocabulary\n",
    "- Pointer model without coverage mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN dataset which was obtained using [Code to obtain the CNN / Daily Mail dataset (non-anonymized) for summarization. Accessed: November 20, 2019](https://github.com/abisee/cnn-dailymail).   \n",
    "Data is enoded into binary files and separated into: train, test and validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T17:52:33.552736Z",
     "start_time": "2020-01-11T17:52:33.357524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked  test.bin  train.bin  val.bin  vocab\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/denys/Code/ML/courses/3_semester/Seminar/dataset/finished_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from encode binary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:14:33.834366Z",
     "start_time": "2020-01-11T17:59:39.668449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R\u001b[7m^^^@^@^@^@^@^@\u001b[m\n",
      "\u001b[7m<CF>\u001b[m<\n",
      "\u001b[7m<F0>^B\u001b[m\n",
      "\u001b[7m^H\u001b[mabstract\u001b[7m^R<E3>^B\u001b[m\n",
      "\u001b[7m<E0>^B\u001b[m\n",
      "\u001b[7m<DD>^B\u001b[m<s> marseille prosecutor says `` so far no videos were used in the crash investigation '' despite media reports . </s> <s> journalists at bild and paris match are `` very confident '' the video clip is real , an editor says . </s> <s> andreas lubitz had informed his lufthansa training school of an episode of severe depression , airline says . </s>\n",
      "\u001b[7m<D9>\u001b[m9\n",
      "\u001b[7m^G\u001b[marticle\u001b[7m^R<CD>\u001b[m9\n",
      "\u001b[7m<CA>\u001b[m9\n",
      "\u001b[K:\u001b[K>\u001b[m9marseille , france -lrb- cnn -rrb- the french prosecutor leading an investigation into the crash of germanwings flight 9525 insisted wednesday that he was not aware of any video footage from on board the plane . marseille prosecutor brice robin told cnn that `` so far no videos were used in the crash investigation . '' he added , `` a person who has such a video needs to immediately give it to the investigators . '' robin 's comments follow claims by two magazines , german daily bild and french paris match , of a cell phone video showing the harrowing final seconds from on board germanwings flight 9525 as it crashed into the french alps . all 150 on board were killed . paris match and bild reported that the video was recovered from a phone at the wreckage site . the two publications d\u001b[7mCode/ML/courses/3_semester/Seminar/dataset/finished_files/chunked/test_000.bin\u001b[m\u001b[K\u0007"
     ]
    }
   ],
   "source": [
    "!less /home/denys/Code/ML/courses/3_semester/Seminar/dataset/finished_files/chunked/test_000.bin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to upload data into my model, I used code referenced in the original paper: \"Abigail See, Peter J. Liu, Christopher D. Manning. Get To The Point: Summarization with Pointer-Generator Networks, 2017\", [Code for the ACL 2017 paper \"Get To The Point: Summarization with Pointer-Generator Networks\". Accessed: November 28, 2019](https://github.com/abisee/pointer-generator). I took specific part of data loading pipeline and modifiend them for my need.   \n",
    "- The logic behind data loading is organased into classes and is quite cumbersome. There such classes as: Vocab, Example, Batch, Batcher which are connected in very \"interesting\" way. Moreover the code is writen using Python2...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T17:44:01.222114Z",
     "start_time": "2020-01-11T17:43:53.569829Z"
    },
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data_util import config\n",
    "from data_util.batcher import Batcher, Batch\n",
    "from data_util.data import Vocab, example_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Vocab(object):\n",
    "  \"\"\"Vocabulary class for mapping between words and ids (integers)\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    \"\"\"Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n",
    "\n",
    "    Args:\n",
    "      vocab_file: path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n",
    "      max_size: integer. The maximum size of the resulting Vocabulary.\"\"\"\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T17:44:05.172898Z",
     "start_time": "2020-01-11T17:44:05.141548Z"
    },
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 5000; we now have 5000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 5000 total words. Last word added: 1980\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary file and specify the size of vocabulary\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# Create Batcher instance, which loads train data into queue in parallel way, encode data using vocabulary \n",
    "train_batcher = Batcher(config.train_data_path,\n",
    "                vocab,\n",
    "                hps=config.hps,\n",
    "                single_pass=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "  def next_batch(self):\n",
    "    # If the batch queue is empty, print a warning\n",
    "    while True:\n",
    "      if self._batch_queue.qsize() == 0:\n",
    "        tf.logging.warning(\n",
    "          'Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i',\n",
    "          self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "        if self._single_pass and self._finished_reading:\n",
    "          tf.logging.info(\"Finished reading dataset in single_pass mode.\")\n",
    "          return False\n",
    "\n",
    "      batch = self._batch_queue.get()  # get the next Batch\n",
    "      enc_batch = batch.enc_batch\n",
    "      target_batch = np.array(list([to_categorical(x, num_classes=self._hps.vocab_size) for x in batch.target_batch]))\n",
    "      yield enc_batch, target_batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First array is encoded original text with words from vocabulary. Second - categorical encoded summary of the corresponded summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:17:21.331167Z",
     "start_time": "2020-01-11T18:17:21.304498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 315,  312,  313,    0,   40,  161,    6,    0, 2786,  502,    9,\n",
       "            0,   10,   12,    0,    0, 1403,   20,    4,  777],\n",
       "        [ 315,  312,  313,   12, 4761,  789,   45, 1723,    8, 4325,   12,\n",
       "            0,  370,    9,    0,   69,   12, 3134,  610,   12],\n",
       "        [ 315,  312,  313,    4, 4816,  210,    0,    0,    6,  612,    0,\n",
       "           11, 2452,    9,    4, 1054,  963,   17, 1407,    9],\n",
       "        [ 315,  312,  313,   82, 1320,    0,  565,  202,  672,    5,   12,\n",
       "            0,    0,   10,  482,  352,   45,  441,  325,   20]],\n",
       "       dtype=int32), array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method next_batch is an generator, which yield batch(original text[x] and summary[y]) with specific batch_size\n",
    "next(train_batcher.next_batch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall new layer look in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Activation, Permute\n",
    "from keras.layers import Input, Flatten, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from data_util import config\n",
    "from .custom_recurrents import AttentionDecoder\n",
    "\n",
    "\n",
    "def PointerModel(num_embeddings=config.vocab_size -1, #5000\n",
    "                 embedding_dim=config.emb_dim,        #128\n",
    "                 n_labels=config.vocab_size -1,       #4999\n",
    "                 pad_length=config.padding,           #20\n",
    "                 encoder_units=config.hidden_dim,     #256\n",
    "                 decoder_units=config.hidden_dim,     #256\n",
    "                 trainable=True,\n",
    "                 return_probabilities=False):\n",
    "\n",
    "    input_ = Input(shape=(pad_length,), dtype='float32')\n",
    "    input_embed = Embedding(num_embeddings, embedding_dim,\n",
    "                            input_length=pad_length,\n",
    "                            trainable=trainable,\n",
    "                            name='OneHot'\n",
    "                            )(input_)\n",
    "\n",
    "    encoder = Bidirectional(LSTM(output_dim=encoder_units, return_sequences=True),\n",
    "                            name='encoder',\n",
    "                            merge_mode='concat',\n",
    "                            trainable=trainable)(input_embed)\n",
    "\n",
    "    decoder = AttentionDecoder(decoder_units,\n",
    "                               name='attention_decoder_1',\n",
    "                               output_dim=n_labels,\n",
    "                               return_probabilities=return_probabilities,\n",
    "                               trainable=trainable)(encoder)\n",
    "    output_2 = Dense(output_dim=n_labels, activation='softmax')(decoder)\n",
    "    model = Model(input=input_, output=output_2)\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main logic from AttentionDecoder class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:30:41.983029Z",
     "start_time": "2020-01-11T18:30:41.968124Z"
    }
   },
   "source": [
    "```python\n",
    "def step(self, x, states):\n",
    "        # 1. Attention Distribution\n",
    "        ytm, stm = states\n",
    "        print(\"stm\", stm.shape)\n",
    "        print(\"ytm\", ytm.shape)\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Waxstm = K.dot(_stm, self.W_a)\n",
    "        _UaxH = time_distributed_dense(self.x_seq, self.U_a,\n",
    "                                       b=self.b_a,\n",
    "                                       input_dim=self.input_dim,\n",
    "                                       timesteps=self.timesteps,\n",
    "                                       output_dim=self.units)\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Waxstm + _UaxH),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        print(\"E_tj\", et.shape)\n",
    "        p_j = K.exp(et)\n",
    "        p_j_sum = K.sum(p_j, axis=1)\n",
    "        p_j_sum_repeated = K.repeat(p_j_sum, self.timesteps)\n",
    "        p_j /= p_j_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # 2. Vocabulary distribution\n",
    "        # calculate the context vector\n",
    "        v_j = K.squeeze(K.batch_dot(p_j, self.x_seq, axes=1), axis=1)\n",
    "        stm_v_j = K.concatenate([stm, v_j])\n",
    "        Vxstm_v_j = K.dot(stm_v_j, self.V)\n",
    "        Vxstm_v_j += self.b\n",
    "        p_vocab = activations.softmax(K.dot(Vxstm_v_j, self.V_) + self.b_)\n",
    "        # 3. Copy distribution\n",
    "        p_copy = p_j\n",
    "        # 4. Generative distribution\n",
    "        p_gen = activations.sigmoid(\n",
    "            K.dot(ytm, self.w_x)\n",
    "            + K.dot(stm, self.w_s)\n",
    "            + K.dot(v_j, self.w_v)\n",
    "            + self.b)\n",
    "        # 5. Final distribution\n",
    "        p_final = p_gen*p_vocab + (1-p_gen)*p_copy\n",
    "        print(\"p_j\", p_j.shape)\n",
    "        print(\"p_final\", p_final.shape)\n",
    "        print(\"p_gen\", p_gen.shape)\n",
    "        if self.return_probabilities:\n",
    "            return p_j, [p_final, p_gen]\n",
    "        else:\n",
    "            return p_final, [p_final, p_gen]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Generator and Reconstructor are seq2seq hybrid pointer-generator networks. I will provide snipets for Generator and Discriminator, because Reconstructor is very similar to th Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Generator(object):\n",
    "    def __init__(self, num_embeddings, embedding_dim, n_labels, pad_length, encoder_units, decoder_units):\n",
    "        self.pointer_model = PointerModel(num_embeddings=num_embeddings,   #4999\n",
    "                                          embedding_dim=embedding_dim,     #128\n",
    "                                          n_labels=n_labels,               #4999\n",
    "                                          pad_length=pad_length,           #20\n",
    "                                          encoder_units=encoder_units,     #256\n",
    "                                          decoder_units=decoder_units,     #256\n",
    "                                          trainable=True,\n",
    "                                          return_probabilities=False)\n",
    "\n",
    "    def model(self):\n",
    "        return self.pointer_model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Discriminator(object):\n",
    "    def __init__(self):\n",
    "        self.vocabulary_size = config.VOCABULARY_SIZE\n",
    "        self.sequence_length = config.MAX_SEQUENCE_LENGTH\n",
    "        self.embedding_dim = config.EMBEDDING_DIM\n",
    "        self.filter_sizes = config.filter_sizes\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.num_filters = config.num_filters\n",
    "        self.output_dim = config.output_dim\n",
    "        self.model_dir = config.models_dir\n",
    "        self.model_path = config.MODEL\n",
    "        self.weights_dir = config.weights_dir\n",
    "        self.weights_path = config.WEIGHTS\n",
    "        self.loss = config.loss\n",
    "        self.optimizer = config.optimizer\n",
    "        self.nb_epoch = config.nb_epoch\n",
    "        self.batch_size = config.batch_size\n",
    "\n",
    "    def model(self):\n",
    "        inputs = Input(shape=(self.sequence_length, self.vocabulary_size))\n",
    "        reshape_1 = Reshape((self.sequence_length, self.vocabulary_size, 1))(inputs)\n",
    "        conv_1_0 = Convolution2D(self.num_filters, \n",
    "                                 self.filter_sizes[0], \n",
    "                                 self.embedding_dim, \n",
    "                                 border_mode='valid', \n",
    "                                 init='normal',\n",
    "                                 activation='relu', \n",
    "                                 dim_ordering='tf')(reshape_1)\n",
    "        maxpool_1_0 = MaxPooling2D(pool_size=(self.sequence_length - self.filter_sizes[0] + 1, 1), \n",
    "                                   strides=(1, 1),\n",
    "                                   border_mode='valid',\n",
    "                                   dim_ordering='tf')(conv_1_0)\n",
    "        conv_1_1 = Convolution2D(self.num_filters, \n",
    "                                 self.filter_sizes[1], \n",
    "                                 self.embedding_dim, \n",
    "                                 border_mode='valid', \n",
    "                                 init='normal',\n",
    "                                 activation='relu', dim_ordering='tf')(reshape_1)\n",
    "        maxpool_1_1 = MaxPooling2D(pool_size=(self.sequence_length - self.filter_sizes[1] + 1, 1), \n",
    "                                   strides=(1, 1),\n",
    "                                   border_mode='valid', \n",
    "                                   dim_ordering='tf')(conv_1_1)\n",
    "        conv_1_2 = Convolution2D(self.num_filters, \n",
    "                                 self.filter_sizes[2],\n",
    "                                 self.embedding_dim, \n",
    "                                 border_mode='valid', \n",
    "                                 init='normal',\n",
    "                                 activation='relu', \n",
    "                                 dim_ordering='tf')(reshape_1)\n",
    "        maxpool_1_2 = MaxPooling2D(pool_size=(self.sequence_length - self.filter_sizes[2] + 1, 1), \n",
    "                                   strides=(1, 1),\n",
    "                                   border_mode='valid', \n",
    "                                   dim_ordering='tf')(conv_1_2)\n",
    "        merged_tensor_1 = merge([maxpool_1_0, maxpool_1_1, maxpool_1_2], mode='concat', concat_axis=1)\n",
    "        flatten_1 = Flatten()(merged_tensor_1)\n",
    "        dropout_1 = Dropout(self.dropout_rate)(flatten_1)\n",
    "        output_1 = Dense(output_dim=self.output_dim, activation='linear')(dropout_1)\n",
    "        model_1 = Model(input=[inputs], output=output_1)\n",
    "        return model_1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T19:57:51.608662Z",
     "start_time": "2020-01-11T19:57:42.276525Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from run import Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T19:57:52.272265Z",
     "start_time": "2020-01-11T19:57:51.610306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 5000; we now have 5000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 5000 total words. Last word added: 1980\n"
     ]
    }
   ],
   "source": [
    "train_model = Train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train generator separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T19:45:51.869346Z",
     "start_time": "2020-01-11T19:43:37.133039Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/pointer_model/model.py:27: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, units=256)`\n",
      "  encoder = Bidirectional(LSTM(output_dim=encoder_units, return_sequences=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (?, ?, 512)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "OneHot (Embedding)           (None, 20, 128)           640000    \n",
      "_________________________________________________________________\n",
      "encoder (Bidirectional)      (None, 20, 512)           788480    \n",
      "_________________________________________________________________\n",
      "attention_decoder_1 (Attenti (None, 20, 5000)          33603784  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20, 5000)          25005000  \n",
      "=================================================================\n",
      "Total params: 60,037,264\n",
      "Trainable params: 60,037,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/pointer_model/model.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=5000)`\n",
      "  output_2 = Dense(output_dim=n_labels, activation='softmax')(decoder)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/pointer_model/model.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  model = Model(input=input_, output=output_2)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/run.py:63: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "  nb_epoch=config.max_iterations)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/run.py:63: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., validation_data=<generator..., callbacks=[<keras.ca..., verbose=1, steps_per_epoch=5, epochs=5, validation_steps=1)`\n",
      "  nb_epoch=config.max_iterations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Compiled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denys/miniconda3/envs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2250: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 15s - loss: 8.4922 - acc: 0.0125 - val_loss: 8.4577 - val_acc: 0.0500\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 13s - loss: 8.4125 - acc: 0.0375 - val_loss: 8.3813 - val_acc: 0.0250\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 13s - loss: 8.3537 - acc: 0.0550 - val_loss: 8.3254 - val_acc: 0.1875\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 13s - loss: 8.3254 - acc: 0.1500 - val_loss: 8.3267 - val_acc: 0.1250\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 13s - loss: 8.3091 - acc: 0.0925 - val_loss: 8.2876 - val_acc: 0.0375\n",
      "Generator training complete.\n"
     ]
    }
   ],
   "source": [
    "train_model.setup_train_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class WGAN(GAN):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WGAN, self).__init__(**kwargs)\n",
    "        self.critic = self.define_critic()\n",
    "        self.gan = self.define_gan()\n",
    "\n",
    "    # calculate wasserstein loss\n",
    "    def wasserstein_loss(srlf, y_true, y_pred):\n",
    "        return backend.mean(y_true * y_pred)\n",
    "\n",
    "    # define the standalone critic model\n",
    "    def define_critic(self):\n",
    "        model = self.discriminator\n",
    "        opt = RMSprop(lr=0.00005)\n",
    "        model.compile(loss=self.wasserstein_loss, optimizer=opt)\n",
    "        return model\n",
    "\n",
    "    # define the combined generator and critic model, for updating the generator\n",
    "    def define_gan(self):\n",
    "        # make weights in the critic not trainable\n",
    "        self.critic.trainable = False\n",
    "        # connect them\n",
    "        model = Sequential()\n",
    "        # add generator\n",
    "        model.add(self.generator)\n",
    "        # add the critic\n",
    "        model.add(self.critic)\n",
    "        # compile model\n",
    "        opt = RMSprop(lr=0.00005)\n",
    "        model.compile(loss=self.wasserstein_loss, optimizer=opt)\n",
    "        return model\n",
    "\n",
    "    # select real samples\n",
    "    def generate_real_samples(self, bath_generator):\n",
    "        # choose random instances\n",
    "        _, X = next(bath_generator)\n",
    "        n_samples = X.shape[0]\n",
    "        # generate class labels, -1 for 'real'\n",
    "        y = -ones((n_samples, 1))\n",
    "        return X, y\n",
    "\n",
    "    # use the generator to generate n fake examples, with class labels\n",
    "    def generate_fake_samples(self, batch_generator):\n",
    "        # generate points in latent space\n",
    "        X, _ = next(batch_generator)\n",
    "        # predict outputs\n",
    "        X = self.generate(X)\n",
    "        n_samples = X.shape[0]\n",
    "        # create class labels with 1.0 for 'fake'\n",
    "        y = ones((n_samples, 1))\n",
    "        return X, y\n",
    "\n",
    "    # train the generator and critic\n",
    "    def train(self, batch_generator, n_steps=200, n_batch=4, n_critic=5, save_iter=20):\n",
    "        c1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "        # manually enumerate epochs\n",
    "        for i in range(n_steps):\n",
    "            # update the critic more than the generator\n",
    "            c1_tmp, c2_tmp = list(), list()\n",
    "            for _ in range(n_critic):\n",
    "                # get randomly selected 'real' samples\n",
    "                X_real, y_real = self.generate_real_samples(batch_generator)\n",
    "                # update critic model weights\n",
    "                c_loss1 = self.critic.train_on_batch(X_real, y_real)\n",
    "                c1_tmp.append(c_loss1)\n",
    "                # generate 'fake' examples\n",
    "                X_fake, y_fake = self.generate_fake_samples(batch_generator)\n",
    "                # update critic model weights\n",
    "                c_loss2 = self.critic.train_on_batch(X_fake, y_fake)\n",
    "                c2_tmp.append(c_loss2)\n",
    "            # store critic loshalf_batchs\n",
    "            c1_hist.append(mean(c1_tmp))\n",
    "            c2_hist.append(mean(c2_tmp))\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan, _ = next(batch_generator)\n",
    "            y_gan = -ones((n_batch, 1))\n",
    "            # update the generator via the critic's error\n",
    "            g_loss = self.gan.train_on_batch(X_gan, y_gan)\n",
    "            g_hist.append(g_loss)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i + 1, c1_hist[-1], c2_hist[-1], g_loss))\n",
    "            if i%save_iter == 0:\n",
    "                print(f\"Real input:{X_gan}\")\n",
    "                samples = self.generate(X_gan)\n",
    "                print(f\"GAN results:{samples} after iteration:{i}\", )\n",
    "        self.plot_history(c1_hist, c2_hist, g_hist)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def setup_train_wgan_model(self):\n",
    "    generator = Generator(num_embeddings=config.vocab_size,  # 4999\n",
    "                          embedding_dim=config.emb_dim,  # 128\n",
    "                          n_labels=config.vocab_size,  # 4999\n",
    "                          pad_length=config.padding,  # 20\n",
    "                          encoder_units=config.hidden_dim,  # 256\n",
    "                          decoder_units=config.hidden_dim,  # 256\n",
    "                          ).model()\n",
    "    reconstructor = Reconstructor(num_embeddings=config.vocab_size,  # 4999\n",
    "                                  embedding_dim=config.emb_dim,  # 128\n",
    "                                  n_labels=config.vocab_size,  # 4999\n",
    "                                  pad_length=config.padding,  # 20\n",
    "                                  encoder_units=config.hidden_dim,  # 256\n",
    "                                  decoder_units=config.hidden_dim,  # 256\n",
    "                                  ).model()\n",
    "    discriminator = Discriminator().model()\n",
    "    wgan = WGAN(generator=generator,\n",
    "                reconstructor=reconstructor,\n",
    "                discriminator=discriminator,\n",
    "                )\n",
    "    try:\n",
    "        wgan.train(self.train_batcher.next_batch())\n",
    "    except KeyboardInterrupt as e:\n",
    "        print('WGAN training stopped early.')\n",
    "    print('WGAN training complete.')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T19:59:42.745458Z",
     "start_time": "2020-01-11T19:57:52.274185Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/pointer_model/model.py:27: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, units=256)`\n",
      "  encoder = Bidirectional(LSTM(output_dim=encoder_units, return_sequences=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (?, ?, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/pointer_model/model.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=5000)`\n",
      "  output_2 = Dense(output_dim=n_labels, activation='softmax')(decoder)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/pointer_model/model.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  model = Model(input=input_, output=output_2)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/GAN_components.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 5000), activation=\"relu\", padding=\"valid\", data_format=\"channels_last\", kernel_initializer=\"normal\")`\n",
      "  activation='relu', dim_ordering='tf')(reshape_1)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/GAN_components.py:70: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(18, 1), strides=(1, 1), padding=\"valid\", data_format=\"channels_last\")`\n",
      "  border_mode='valid', dim_ordering='tf')(conv_1_0)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/GAN_components.py:72: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (4, 5000), activation=\"relu\", padding=\"valid\", data_format=\"channels_last\", kernel_initializer=\"normal\")`\n",
      "  activation='relu', dim_ordering='tf')(reshape_1)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/GAN_components.py:74: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(17, 1), strides=(1, 1), padding=\"valid\", data_format=\"channels_last\")`\n",
      "  border_mode='valid', dim_ordering='tf')(conv_1_1)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/GAN_components.py:76: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5000), activation=\"relu\", padding=\"valid\", data_format=\"channels_last\", kernel_initializer=\"normal\")`\n",
      "  activation='relu', dim_ordering='tf')(reshape_1)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/GAN_components.py:78: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(16, 1), strides=(1, 1), padding=\"valid\", data_format=\"channels_last\")`\n",
      "  border_mode='valid', dim_ordering='tf')(conv_1_2)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/GAN_components.py:80: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  merged_tensor_1 = merge([maxpool_1_0, maxpool_1_1, maxpool_1_2], mode='concat', concat_axis=1)\n",
      "/home/denys/miniconda3/envs/ml/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/GAN_components.py:85: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1)`\n",
      "  output_1 = Dense(output_dim=self.output_dim, activation='linear')(dropout_1)\n",
      "/home/denys/Code/ML/courses/3_semester/Seminar/seminar/models/GAN_components.py:86: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  model_1 = Model(input=[inputs], output=output_1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "OneHot (Embedding)           (None, 20, 128)           640000    \n",
      "_________________________________________________________________\n",
      "encoder (Bidirectional)      (None, 20, 512)           788480    \n",
      "_________________________________________________________________\n",
      "attention_decoder_1 (Attenti (None, 20, 5000)          33603784  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20, 5000)          25005000  \n",
      "=================================================================\n",
      "Total params: 60,037,264\n",
      "Trainable params: 60,037,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 20, 5000)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 20, 5000, 1)   0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 18, 1, 32)     480032      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 17, 1, 32)     640032      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 16, 1, 32)     800032      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 1, 1, 32)      0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 1, 1, 32)      0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 1, 1, 32)      0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 3, 1, 32)      0           max_pooling2d_1[0][0]            \n",
      "                                                                   max_pooling2d_2[0][0]            \n",
      "                                                                   max_pooling2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 96)            0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 96)            0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             97          dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,920,193\n",
      "Trainable params: 1,920,193\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_1 (Model)              (None, 20, 5000)          60037264  \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 1)                 1920193   \n",
      "=================================================================\n",
      "Total params: 61,957,457\n",
      "Trainable params: 61,957,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      ">1, c1=-0.393, c2=-0.001 g=0.005\n",
      "Real input:[[ 315  312  313   24    7   12 1855    0  985 2774    0    5  311    7\n",
      "    12  468  432    0   25  135]\n",
      " [ 315  312  313    0    0   62    0  340  354    8    4    0    9    4\n",
      "     0    0  710    5   17   25]\n",
      " [ 882  936  315  312  313   13    7  318  329   44   63  352    7 2538\n",
      "   685    5    4 2016  612    0]\n",
      " [ 315  312  313   42 1152    0    6   39 1152    4  536  332  184   69\n",
      "  1087 1779 2968 3406    8    4]]\n",
      "GAN results:[[[0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  ...\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]]\n",
      "\n",
      " [[0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  ...\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]]\n",
      "\n",
      " [[0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  ...\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]]\n",
      "\n",
      " [[0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  ...\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]\n",
      "  [0.00020005 0.00020001 0.00020006 ... 0.00020003 0.00020009 0.00020006]]] after iteration:0\n",
      ">2, c1=-0.352, c2=-0.007 g=0.008\n",
      ">3, c1=-0.463, c2=-0.010 g=0.013\n",
      ">4, c1=-0.354, c2=-0.012 g=0.014\n",
      ">5, c1=-0.394, c2=-0.015 g=0.017\n",
      ">6, c1=-0.459, c2=-0.016 g=0.018\n",
      "WGAN training stopped early.\n",
      "WGAN training complete.\n"
     ]
    }
   ],
   "source": [
    "train_model.setup_train_wgan_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T20:14:28.017440Z",
     "start_time": "2020-01-11T20:14:28.005733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./plot_line_plot_loss.png\",width=60,height=60>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"./plot_line_plot_loss.png\",width=60,height=60>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
